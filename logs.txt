
==> Audit <==
|------------|------------------------|----------|-----------|---------|---------------------|---------------------|
|  Command   |          Args          | Profile  |   User    | Version |     Start Time      |      End Time       |
|------------|------------------------|----------|-----------|---------|---------------------|---------------------|
| start      |                        | minikube | MSI\prati | v1.36.0 | 29 May 25 21:17 IST | 29 May 25 21:30 IST |
| service    | flaskapp-service       | minikube | MSI\prati | v1.36.0 | 29 May 25 22:35 IST |                     |
| service    | flaskapp-service --url | minikube | MSI\prati | v1.36.0 | 29 May 25 22:35 IST |                     |
| service    | flaskapp-service       | minikube | MSI\prati | v1.36.0 | 29 May 25 22:37 IST |                     |
| start      |                        | minikube | MSI\prati | v1.36.0 | 30 May 25 20:17 IST |                     |
| docker-env |                        | minikube | MSI\prati | v1.36.0 | 30 May 25 20:22 IST | 30 May 25 20:23 IST |
| start      |                        | minikube | MSI\prati | v1.36.0 | 30 May 25 20:24 IST |                     |
| delete     |                        | minikube | MSI\prati | v1.36.0 | 30 May 25 20:25 IST | 30 May 25 20:25 IST |
| start      | --driver=docker        | minikube | MSI\prati | v1.36.0 | 30 May 25 20:25 IST | 30 May 25 20:26 IST |
| service    | flaskapp-service       | minikube | MSI\prati | v1.36.0 | 30 May 25 20:28 IST |                     |
|------------|------------------------|----------|-----------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/05/30 20:25:46
Running on machine: MSI
Binary: Built with gc go1.24.0 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0530 20:25:46.764860    6964 out.go:345] Setting OutFile to fd 500 ...
I0530 20:25:46.764860    6964 out.go:397] isatty.IsTerminal(500) = true
I0530 20:25:46.764860    6964 out.go:358] Setting ErrFile to fd 500...
I0530 20:25:46.764860    6964 out.go:397] isatty.IsTerminal(500) = true
I0530 20:25:46.781738    6964 out.go:352] Setting JSON to false
I0530 20:25:46.784235    6964 start.go:130] hostinfo: {"hostname":"MSI","uptime":14576,"bootTime":1748602369,"procs":221,"os":"windows","platform":"Microsoft Windows 11 Home Single Language","platformFamily":"Standalone Workstation","platformVersion":"10.0.22631.5413 Build 22631.5413","kernelVersion":"10.0.22631.5413 Build 22631.5413","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"71a739a0-8482-4aea-a98a-6658a5c28a27"}
W0530 20:25:46.784331    6964 start.go:138] gopshost.Virtualization returned error: not implemented yet
I0530 20:25:46.786406    6964 out.go:177] 😄  minikube v1.36.0 on Microsoft Windows 11 Home Single Language 10.0.22631.5413 Build 22631.5413
I0530 20:25:46.789048    6964 notify.go:220] Checking for updates...
I0530 20:25:46.790197    6964 driver.go:404] Setting default libvirt URI to qemu:///system
I0530 20:25:46.846581    6964 docker.go:123] docker version: linux-28.1.1:Docker Desktop 4.41.2 (191736)
I0530 20:25:46.849376    6964 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0530 20:25:47.067401    6964 info.go:266] docker info: {ID:020289c8-dbc9-43a1-8f3c-dc4a76084aac Containers:3 ContainersRunning:1 ContainersPaused:0 ContainersStopped:2 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:74 OomKillDisable:true NGoroutines:95 SystemTime:2025-05-30 14:55:47.050745783 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4028653568 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.7] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.3.0] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.8] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:dev] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.11] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.1]] Warnings:<nil>}}
I0530 20:25:47.069035    6964 out.go:177] ✨  Using the docker driver based on user configuration
I0530 20:25:47.071154    6964 start.go:304] selected driver: docker
I0530 20:25:47.071154    6964 start.go:908] validating driver "docker" against <nil>
I0530 20:25:47.071154    6964 start.go:919] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0530 20:25:47.080422    6964 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0530 20:25:47.275244    6964 info.go:266] docker info: {ID:020289c8-dbc9-43a1-8f3c-dc4a76084aac Containers:3 ContainersRunning:1 ContainersPaused:0 ContainersStopped:2 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:76 OomKillDisable:true NGoroutines:99 SystemTime:2025-05-30 14:55:47.262342134 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:18 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4028653568 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.7] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.3.0] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.8] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:dev] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.11] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.1]] Warnings:<nil>}}
I0530 20:25:47.275763    6964 start_flags.go:311] no existing cluster config was found, will generate one from the flags 
I0530 20:25:47.330116    6964 start_flags.go:394] Using suggested 2200MB memory alloc based on sys=8036MB, container=3842MB
I0530 20:25:47.330636    6964 start_flags.go:958] Wait components to verify : map[apiserver:true system_pods:true]
I0530 20:25:47.332360    6964 out.go:177] 📌  Using Docker Desktop driver with root privileges
I0530 20:25:47.333606    6964 cni.go:84] Creating CNI manager for ""
I0530 20:25:47.333606    6964 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0530 20:25:47.333606    6964 start_flags.go:320] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0530 20:25:47.334112    6964 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\prati:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0530 20:25:47.335429    6964 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0530 20:25:47.337744    6964 cache.go:121] Beginning downloading kic base image for docker with docker
I0530 20:25:47.338930    6964 out.go:177] 🚜  Pulling base image v0.0.47 ...
I0530 20:25:47.341128    6964 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0530 20:25:47.341128    6964 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon
I0530 20:25:47.341128    6964 preload.go:146] Found local preload: C:\Users\prati\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4
I0530 20:25:47.341128    6964 cache.go:56] Caching tarball of preloaded images
I0530 20:25:47.341737    6964 preload.go:172] Found C:\Users\prati\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0530 20:25:47.341737    6964 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0530 20:25:47.342857    6964 profile.go:143] Saving config to C:\Users\prati\.minikube\profiles\minikube\config.json ...
I0530 20:25:47.342857    6964 lock.go:35] WriteFile acquiring C:\Users\prati\.minikube\profiles\minikube\config.json: {Name:mkef9005d7ac853bab7c0d9d24e8c596ce0e9218 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0530 20:25:47.415533    6964 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon, skipping pull
I0530 20:25:47.415533    6964 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b exists in daemon, skipping load
I0530 20:25:47.415533    6964 cache.go:230] Successfully downloaded all kic artifacts
I0530 20:25:47.415533    6964 start.go:360] acquireMachinesLock for minikube: {Name:mk7ec82aead5d89337dd3bee58dd59b21c6e3637 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0530 20:25:47.415533    6964 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0530 20:25:47.415533    6964 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\prati:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0530 20:25:47.416063    6964 start.go:125] createHost starting for "" (driver="docker")
I0530 20:25:47.418949    6964 out.go:235] 🔥  Creating docker container (CPUs=2, Memory=2200MB) ...
I0530 20:25:47.420806    6964 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0530 20:25:47.420806    6964 client.go:168] LocalClient.Create starting
I0530 20:25:47.421382    6964 main.go:141] libmachine: Reading certificate data from C:\Users\prati\.minikube\certs\ca.pem
I0530 20:25:47.432108    6964 main.go:141] libmachine: Decoding PEM data...
I0530 20:25:47.432108    6964 main.go:141] libmachine: Parsing certificate...
I0530 20:25:47.433392    6964 main.go:141] libmachine: Reading certificate data from C:\Users\prati\.minikube\certs\cert.pem
I0530 20:25:47.441483    6964 main.go:141] libmachine: Decoding PEM data...
I0530 20:25:47.441483    6964 main.go:141] libmachine: Parsing certificate...
I0530 20:25:47.445861    6964 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0530 20:25:47.487002    6964 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0530 20:25:47.489266    6964 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0530 20:25:47.489266    6964 cli_runner.go:164] Run: docker network inspect minikube
W0530 20:25:47.523618    6964 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0530 20:25:47.523618    6964 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0530 20:25:47.523618    6964 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0530 20:25:47.525713    6964 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0530 20:25:47.585781    6964 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc0016d1230}
I0530 20:25:47.585781    6964 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0530 20:25:47.588570    6964 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0530 20:25:47.665956    6964 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0530 20:25:47.666583    6964 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0530 20:25:47.675447    6964 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0530 20:25:47.716805    6964 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0530 20:25:47.754529    6964 oci.go:103] Successfully created a docker volume minikube
I0530 20:25:47.757221    6964 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -d /var/lib
I0530 20:25:49.222691    6964 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -d /var/lib: (1.4649597s)
I0530 20:25:49.222691    6964 oci.go:107] Successfully prepared a docker volume minikube
I0530 20:25:49.222781    6964 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0530 20:25:49.222781    6964 kic.go:194] Starting extracting preloaded images to volume ...
I0530 20:25:49.226068    6964 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\prati\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -I lz4 -xf /preloaded.tar -C /extractDir
I0530 20:26:02.899736    6964 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\prati\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -I lz4 -xf /preloaded.tar -C /extractDir: (13.6736685s)
I0530 20:26:02.899736    6964 kic.go:203] duration metric: took 13.6769551s to extract preloaded images to volume ...
I0530 20:26:02.903897    6964 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0530 20:26:04.209601    6964 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.3057043s)
I0530 20:26:04.212828    6964 info.go:266] docker info: {ID:020289c8-dbc9-43a1-8f3c-dc4a76084aac Containers:3 ContainersRunning:1 ContainersPaused:0 ContainersStopped:2 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:74 OomKillDisable:true NGoroutines:92 SystemTime:2025-05-30 14:56:04.190139817 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4028653568 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.7] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.3.0] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.8] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:dev] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.11] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.1]] Warnings:<nil>}}
I0530 20:26:04.215608    6964 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0530 20:26:04.438110    6964 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b
I0530 20:26:04.961354    6964 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0530 20:26:05.022244    6964 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0530 20:26:05.083119    6964 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0530 20:26:05.216604    6964 oci.go:144] the created container "minikube" has a running status.
I0530 20:26:05.217128    6964 kic.go:225] Creating ssh key for kic: C:\Users\prati\.minikube\machines\minikube\id_rsa...
I0530 20:26:05.345642    6964 kic_runner.go:191] docker (temp): C:\Users\prati\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0530 20:26:05.435768    6964 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0530 20:26:05.504365    6964 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0530 20:26:05.504365    6964 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0530 20:26:05.610482    6964 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\prati\.minikube\machines\minikube\id_rsa...
W0530 20:26:05.610482    6964 kic.go:271] unable to determine current user's SID. minikube tunnel may not work.
I0530 20:26:05.618085    6964 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0530 20:26:05.674303    6964 machine.go:93] provisionDockerMachine start ...
I0530 20:26:05.680411    6964 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0530 20:26:05.732272    6964 main.go:141] libmachine: Using SSH client type: native
I0530 20:26:05.750762    6964 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x45a9e0] 0x45d520 <nil>  [] 0s} 127.0.0.1 51206 <nil> <nil>}
I0530 20:26:05.750762    6964 main.go:141] libmachine: About to run SSH command:
hostname
I0530 20:26:05.753142    6964 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0530 20:26:08.914136    6964 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0530 20:26:08.915222    6964 ubuntu.go:169] provisioning hostname "minikube"
I0530 20:26:08.917925    6964 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0530 20:26:08.961183    6964 main.go:141] libmachine: Using SSH client type: native
I0530 20:26:08.961741    6964 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x45a9e0] 0x45d520 <nil>  [] 0s} 127.0.0.1 51206 <nil> <nil>}
I0530 20:26:08.961741    6964 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0530 20:26:09.143066    6964 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0530 20:26:09.148807    6964 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0530 20:26:09.193448    6964 main.go:141] libmachine: Using SSH client type: native
I0530 20:26:09.193966    6964 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x45a9e0] 0x45d520 <nil>  [] 0s} 127.0.0.1 51206 <nil> <nil>}
I0530 20:26:09.193966    6964 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0530 20:26:09.329214    6964 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0530 20:26:09.329786    6964 ubuntu.go:175] set auth options {CertDir:C:\Users\prati\.minikube CaCertPath:C:\Users\prati\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\prati\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\prati\.minikube\machines\server.pem ServerKeyPath:C:\Users\prati\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\prati\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\prati\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\prati\.minikube}
I0530 20:26:09.329786    6964 ubuntu.go:177] setting up certificates
I0530 20:26:09.329786    6964 provision.go:84] configureAuth start
I0530 20:26:09.332451    6964 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0530 20:26:09.376575    6964 provision.go:143] copyHostCerts
I0530 20:26:09.378385    6964 exec_runner.go:144] found C:\Users\prati\.minikube/ca.pem, removing ...
I0530 20:26:09.378385    6964 exec_runner.go:203] rm: C:\Users\prati\.minikube\ca.pem
I0530 20:26:09.378385    6964 exec_runner.go:151] cp: C:\Users\prati\.minikube\certs\ca.pem --> C:\Users\prati\.minikube/ca.pem (1074 bytes)
I0530 20:26:09.379510    6964 exec_runner.go:144] found C:\Users\prati\.minikube/cert.pem, removing ...
I0530 20:26:09.379510    6964 exec_runner.go:203] rm: C:\Users\prati\.minikube\cert.pem
I0530 20:26:09.380016    6964 exec_runner.go:151] cp: C:\Users\prati\.minikube\certs\cert.pem --> C:\Users\prati\.minikube/cert.pem (1119 bytes)
I0530 20:26:09.394722    6964 exec_runner.go:144] found C:\Users\prati\.minikube/key.pem, removing ...
I0530 20:26:09.394722    6964 exec_runner.go:203] rm: C:\Users\prati\.minikube\key.pem
I0530 20:26:09.395227    6964 exec_runner.go:151] cp: C:\Users\prati\.minikube\certs\key.pem --> C:\Users\prati\.minikube/key.pem (1675 bytes)
I0530 20:26:09.395227    6964 provision.go:117] generating server cert: C:\Users\prati\.minikube\machines\server.pem ca-key=C:\Users\prati\.minikube\certs\ca.pem private-key=C:\Users\prati\.minikube\certs\ca-key.pem org=prati.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0530 20:26:09.598406    6964 provision.go:177] copyRemoteCerts
I0530 20:26:09.604333    6964 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0530 20:26:09.617364    6964 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0530 20:26:09.673677    6964 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51206 SSHKeyPath:C:\Users\prati\.minikube\machines\minikube\id_rsa Username:docker}
I0530 20:26:09.795178    6964 ssh_runner.go:362] scp C:\Users\prati\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0530 20:26:09.823905    6964 ssh_runner.go:362] scp C:\Users\prati\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I0530 20:26:09.849655    6964 ssh_runner.go:362] scp C:\Users\prati\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0530 20:26:09.875254    6964 provision.go:87] duration metric: took 544.8961ms to configureAuth
I0530 20:26:09.875254    6964 ubuntu.go:193] setting minikube options for container-runtime
I0530 20:26:09.875830    6964 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0530 20:26:09.878130    6964 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0530 20:26:09.919451    6964 main.go:141] libmachine: Using SSH client type: native
I0530 20:26:09.919525    6964 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x45a9e0] 0x45d520 <nil>  [] 0s} 127.0.0.1 51206 <nil> <nil>}
I0530 20:26:09.919525    6964 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0530 20:26:10.062369    6964 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0530 20:26:10.062369    6964 ubuntu.go:71] root file system type: overlay
I0530 20:26:10.063451    6964 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0530 20:26:10.065645    6964 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0530 20:26:10.113076    6964 main.go:141] libmachine: Using SSH client type: native
I0530 20:26:10.113618    6964 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x45a9e0] 0x45d520 <nil>  [] 0s} 127.0.0.1 51206 <nil> <nil>}
I0530 20:26:10.113618    6964 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0530 20:26:10.276709    6964 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0530 20:26:10.279505    6964 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0530 20:26:10.320453    6964 main.go:141] libmachine: Using SSH client type: native
I0530 20:26:10.324547    6964 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x45a9e0] 0x45d520 <nil>  [] 0s} 127.0.0.1 51206 <nil> <nil>}
I0530 20:26:10.324547    6964 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0530 20:26:14.092859    6964 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2025-04-18 09:50:48.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-05-30 14:56:10.270465157 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0530 20:26:14.092859    6964 machine.go:96] duration metric: took 8.4185566s to provisionDockerMachine
I0530 20:26:14.092859    6964 client.go:171] duration metric: took 26.6720535s to LocalClient.Create
I0530 20:26:14.092859    6964 start.go:167] duration metric: took 26.6720535s to libmachine.API.Create "minikube"
I0530 20:26:14.093405    6964 start.go:293] postStartSetup for "minikube" (driver="docker")
I0530 20:26:14.093405    6964 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0530 20:26:14.097789    6964 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0530 20:26:14.099583    6964 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0530 20:26:14.143017    6964 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51206 SSHKeyPath:C:\Users\prati\.minikube\machines\minikube\id_rsa Username:docker}
I0530 20:26:14.246679    6964 ssh_runner.go:195] Run: cat /etc/os-release
I0530 20:26:14.251592    6964 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0530 20:26:14.251592    6964 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0530 20:26:14.251592    6964 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0530 20:26:14.251592    6964 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0530 20:26:14.251592    6964 filesync.go:126] Scanning C:\Users\prati\.minikube\addons for local assets ...
I0530 20:26:14.252230    6964 filesync.go:126] Scanning C:\Users\prati\.minikube\files for local assets ...
I0530 20:26:14.252230    6964 start.go:296] duration metric: took 158.8247ms for postStartSetup
I0530 20:26:14.255978    6964 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0530 20:26:14.298794    6964 profile.go:143] Saving config to C:\Users\prati\.minikube\profiles\minikube\config.json ...
I0530 20:26:14.301099    6964 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0530 20:26:14.303822    6964 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0530 20:26:14.347542    6964 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51206 SSHKeyPath:C:\Users\prati\.minikube\machines\minikube\id_rsa Username:docker}
I0530 20:26:14.440023    6964 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0530 20:26:14.446473    6964 start.go:128] duration metric: took 27.0304097s to createHost
I0530 20:26:14.446473    6964 start.go:83] releasing machines lock for "minikube", held for 27.0309405s
I0530 20:26:14.449002    6964 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0530 20:26:14.486955    6964 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0530 20:26:14.487480    6964 ssh_runner.go:195] Run: cat /version.json
I0530 20:26:14.490104    6964 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0530 20:26:14.490635    6964 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0530 20:26:14.534502    6964 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51206 SSHKeyPath:C:\Users\prati\.minikube\machines\minikube\id_rsa Username:docker}
I0530 20:26:14.545448    6964 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51206 SSHKeyPath:C:\Users\prati\.minikube\machines\minikube\id_rsa Username:docker}
W0530 20:26:14.638004    6964 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0530 20:26:14.643388    6964 ssh_runner.go:195] Run: systemctl --version
I0530 20:26:14.650191    6964 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0530 20:26:14.659972    6964 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0530 20:26:14.672036    6964 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0530 20:26:14.677201    6964 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0530 20:26:14.714654    6964 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0530 20:26:14.714654    6964 start.go:495] detecting cgroup driver to use...
I0530 20:26:14.714654    6964 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0530 20:26:14.716798    6964 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0530 20:26:14.735897    6964 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0530 20:26:14.748608    6964 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0530 20:26:14.759610    6964 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0530 20:26:14.760384    6964 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0530 20:26:14.772479    6964 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0530 20:26:14.784444    6964 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0530 20:26:14.798604    6964 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0530 20:26:14.812143    6964 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0530 20:26:14.823951    6964 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0530 20:26:14.836643    6964 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0530 20:26:14.849700    6964 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0530 20:26:14.865194    6964 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0530 20:26:14.879309    6964 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0530 20:26:14.893076    6964 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0530 20:26:14.996795    6964 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0530 20:26:15.138992    6964 start.go:495] detecting cgroup driver to use...
I0530 20:26:15.138992    6964 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0530 20:26:15.144191    6964 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0530 20:26:15.159212    6964 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0530 20:26:15.164633    6964 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0530 20:26:15.178931    6964 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0530 20:26:15.198534    6964 ssh_runner.go:195] Run: which cri-dockerd
I0530 20:26:15.208191    6964 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0530 20:26:15.220318    6964 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0530 20:26:15.244335    6964 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0530 20:26:15.367015    6964 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0530 20:26:15.465649    6964 docker.go:587] configuring docker to use "cgroupfs" as cgroup driver...
I0530 20:26:15.466154    6964 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0530 20:26:15.487565    6964 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0530 20:26:15.503754    6964 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0530 20:26:15.625872    6964 ssh_runner.go:195] Run: sudo systemctl restart docker
W0530 20:26:16.713110    6964 out.go:270] ❗  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0530 20:26:16.713907    6964 out.go:270] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0530 20:26:21.251746    6964 ssh_runner.go:235] Completed: sudo systemctl restart docker: (5.6258744s)
I0530 20:26:21.256224    6964 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0530 20:26:21.279793    6964 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0530 20:26:21.304291    6964 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0530 20:26:21.478966    6964 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0530 20:26:21.669592    6964 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0530 20:26:21.821138    6964 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0530 20:26:21.847324    6964 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0530 20:26:21.865722    6964 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0530 20:26:21.970167    6964 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0530 20:26:22.328062    6964 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0530 20:26:22.339786    6964 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0530 20:26:22.340864    6964 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0530 20:26:22.345136    6964 start.go:563] Will wait 60s for crictl version
I0530 20:26:22.346252    6964 ssh_runner.go:195] Run: which crictl
I0530 20:26:22.354268    6964 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0530 20:26:22.526513    6964 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.1.1
RuntimeApiVersion:  v1
I0530 20:26:22.530342    6964 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0530 20:26:22.679048    6964 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0530 20:26:22.705641    6964 out.go:235] 🐳  Preparing Kubernetes v1.33.1 on Docker 28.1.1 ...
I0530 20:26:22.708067    6964 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0530 20:26:22.864524    6964 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0530 20:26:22.867245    6964 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0530 20:26:22.872122    6964 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0530 20:26:22.887560    6964 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0530 20:26:22.959978    6964 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\prati:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0530 20:26:22.970595    6964 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0530 20:26:22.970595    6964 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0530 20:26:23.000010    6964 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0530 20:26:23.000010    6964 docker.go:632] Images already preloaded, skipping extraction
I0530 20:26:23.003472    6964 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0530 20:26:23.024573    6964 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0530 20:26:23.025768    6964 cache_images.go:84] Images are preloaded, skipping loading
I0530 20:26:23.025768    6964 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.33.1 docker true true} ...
I0530 20:26:23.031581    6964 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0530 20:26:23.033537    6964 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0530 20:26:23.317074    6964 cni.go:84] Creating CNI manager for ""
I0530 20:26:23.317074    6964 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0530 20:26:23.317074    6964 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0530 20:26:23.317074    6964 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.33.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0530 20:26:23.317074    6964 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.33.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0530 20:26:23.321352    6964 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0530 20:26:23.333365    6964 binaries.go:44] Found k8s binaries, skipping transfer
I0530 20:26:23.337353    6964 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0530 20:26:23.348844    6964 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0530 20:26:23.368640    6964 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0530 20:26:23.386846    6964 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0530 20:26:23.405203    6964 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0530 20:26:23.410093    6964 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0530 20:26:23.426184    6964 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0530 20:26:23.527180    6964 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0530 20:26:23.543349    6964 certs.go:68] Setting up C:\Users\prati\.minikube\profiles\minikube for IP: 192.168.49.2
I0530 20:26:23.543906    6964 certs.go:194] generating shared ca certs ...
I0530 20:26:23.543906    6964 certs.go:226] acquiring lock for ca certs: {Name:mkd3413a32f011788998d56c95eff8c0ceb52f1b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0530 20:26:23.555862    6964 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\prati\.minikube\ca.key
I0530 20:26:23.578679    6964 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\prati\.minikube\proxy-client-ca.key
I0530 20:26:23.578679    6964 certs.go:256] generating profile certs ...
I0530 20:26:23.580257    6964 certs.go:363] generating signed profile cert for "minikube-user": C:\Users\prati\.minikube\profiles\minikube\client.key
I0530 20:26:23.580257    6964 crypto.go:68] Generating cert C:\Users\prati\.minikube\profiles\minikube\client.crt with IP's: []
I0530 20:26:23.945419    6964 crypto.go:156] Writing cert to C:\Users\prati\.minikube\profiles\minikube\client.crt ...
I0530 20:26:23.945419    6964 lock.go:35] WriteFile acquiring C:\Users\prati\.minikube\profiles\minikube\client.crt: {Name:mk4d18a0aa7e04127b46775d832e5d64bc578e4e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0530 20:26:23.945419    6964 crypto.go:164] Writing key to C:\Users\prati\.minikube\profiles\minikube\client.key ...
I0530 20:26:23.945419    6964 lock.go:35] WriteFile acquiring C:\Users\prati\.minikube\profiles\minikube\client.key: {Name:mkfc9c450a1bdddfdf4aa427bcc1d9155f997f96 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0530 20:26:23.945419    6964 certs.go:363] generating signed profile cert for "minikube": C:\Users\prati\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0530 20:26:23.945419    6964 crypto.go:68] Generating cert C:\Users\prati\.minikube\profiles\minikube\apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0530 20:26:24.466508    6964 crypto.go:156] Writing cert to C:\Users\prati\.minikube\profiles\minikube\apiserver.crt.7fb57e3c ...
I0530 20:26:24.466508    6964 lock.go:35] WriteFile acquiring C:\Users\prati\.minikube\profiles\minikube\apiserver.crt.7fb57e3c: {Name:mkfd9d8485fe1dc7a15765766ab767cbf4088348 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0530 20:26:24.466508    6964 crypto.go:164] Writing key to C:\Users\prati\.minikube\profiles\minikube\apiserver.key.7fb57e3c ...
I0530 20:26:24.466508    6964 lock.go:35] WriteFile acquiring C:\Users\prati\.minikube\profiles\minikube\apiserver.key.7fb57e3c: {Name:mkc401e9ddaf8ffc2cf7ecc1d638920f72ec0665 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0530 20:26:24.466508    6964 certs.go:381] copying C:\Users\prati\.minikube\profiles\minikube\apiserver.crt.7fb57e3c -> C:\Users\prati\.minikube\profiles\minikube\apiserver.crt
I0530 20:26:24.477426    6964 certs.go:385] copying C:\Users\prati\.minikube\profiles\minikube\apiserver.key.7fb57e3c -> C:\Users\prati\.minikube\profiles\minikube\apiserver.key
I0530 20:26:24.477933    6964 certs.go:363] generating signed profile cert for "aggregator": C:\Users\prati\.minikube\profiles\minikube\proxy-client.key
I0530 20:26:24.478942    6964 crypto.go:68] Generating cert C:\Users\prati\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I0530 20:26:24.711458    6964 crypto.go:156] Writing cert to C:\Users\prati\.minikube\profiles\minikube\proxy-client.crt ...
I0530 20:26:24.711458    6964 lock.go:35] WriteFile acquiring C:\Users\prati\.minikube\profiles\minikube\proxy-client.crt: {Name:mk27a4bdfabe5f39c2316e6e3952b8438c2d7798 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0530 20:26:24.711458    6964 crypto.go:164] Writing key to C:\Users\prati\.minikube\profiles\minikube\proxy-client.key ...
I0530 20:26:24.711458    6964 lock.go:35] WriteFile acquiring C:\Users\prati\.minikube\profiles\minikube\proxy-client.key: {Name:mkf98fa13031142f16f8d84a0f2a89e8cfbf545a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0530 20:26:24.722160    6964 certs.go:484] found cert: C:\Users\prati\.minikube\certs\ca-key.pem (1679 bytes)
I0530 20:26:24.722160    6964 certs.go:484] found cert: C:\Users\prati\.minikube\certs\ca.pem (1074 bytes)
I0530 20:26:24.722160    6964 certs.go:484] found cert: C:\Users\prati\.minikube\certs\cert.pem (1119 bytes)
I0530 20:26:24.722160    6964 certs.go:484] found cert: C:\Users\prati\.minikube\certs\key.pem (1675 bytes)
I0530 20:26:24.732518    6964 ssh_runner.go:362] scp C:\Users\prati\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0530 20:26:24.762650    6964 ssh_runner.go:362] scp C:\Users\prati\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0530 20:26:24.790756    6964 ssh_runner.go:362] scp C:\Users\prati\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0530 20:26:24.815482    6964 ssh_runner.go:362] scp C:\Users\prati\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0530 20:26:24.839509    6964 ssh_runner.go:362] scp C:\Users\prati\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0530 20:26:24.866042    6964 ssh_runner.go:362] scp C:\Users\prati\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0530 20:26:24.890941    6964 ssh_runner.go:362] scp C:\Users\prati\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0530 20:26:24.915527    6964 ssh_runner.go:362] scp C:\Users\prati\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0530 20:26:24.940977    6964 ssh_runner.go:362] scp C:\Users\prati\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0530 20:26:24.969919    6964 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0530 20:26:24.990712    6964 ssh_runner.go:195] Run: openssl version
I0530 20:26:25.005997    6964 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0530 20:26:25.020000    6964 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0530 20:26:25.023967    6964 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May 29 16:00 /usr/share/ca-certificates/minikubeCA.pem
I0530 20:26:25.025064    6964 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0530 20:26:25.036819    6964 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0530 20:26:25.050941    6964 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0530 20:26:25.055871    6964 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0530 20:26:25.056916    6964 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\prati:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0530 20:26:25.059279    6964 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0530 20:26:25.088192    6964 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0530 20:26:25.102364    6964 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0530 20:26:25.112587    6964 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0530 20:26:25.117124    6964 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0530 20:26:25.127499    6964 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0530 20:26:25.127499    6964 kubeadm.go:157] found existing configuration files:

I0530 20:26:25.131617    6964 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0530 20:26:25.140246    6964 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0530 20:26:25.145779    6964 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0530 20:26:25.160350    6964 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0530 20:26:25.173688    6964 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0530 20:26:25.178162    6964 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0530 20:26:25.196855    6964 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0530 20:26:25.206997    6964 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0530 20:26:25.211420    6964 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0530 20:26:25.226543    6964 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0530 20:26:25.237324    6964 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0530 20:26:25.241498    6964 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0530 20:26:25.254917    6964 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.33.1:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0530 20:26:25.359429    6964 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0530 20:26:25.374908    6964 kubeadm.go:310] 	[WARNING SystemVerification]: cgroups v1 support is in maintenance mode, please migrate to cgroups v2
I0530 20:26:25.474917    6964 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0530 20:26:46.415875    6964 kubeadm.go:310] [init] Using Kubernetes version: v1.33.1
I0530 20:26:46.415875    6964 kubeadm.go:310] [preflight] Running pre-flight checks
I0530 20:26:46.415875    6964 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0530 20:26:46.415875    6964 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0530 20:26:46.416476    6964 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0530 20:26:46.416476    6964 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0530 20:26:46.419536    6964 out.go:235]     ▪ Generating certificates and keys ...
I0530 20:26:46.420741    6964 kubeadm.go:310] [certs] Using existing ca certificate authority
I0530 20:26:46.420741    6964 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0530 20:26:46.420741    6964 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0530 20:26:46.420741    6964 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0530 20:26:46.421273    6964 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0530 20:26:46.421273    6964 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0530 20:26:46.421273    6964 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0530 20:26:46.421273    6964 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0530 20:26:46.421809    6964 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0530 20:26:46.421809    6964 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0530 20:26:46.421809    6964 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0530 20:26:46.421809    6964 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0530 20:26:46.421809    6964 kubeadm.go:310] [certs] Generating "sa" key and public key
I0530 20:26:46.422337    6964 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0530 20:26:46.422337    6964 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0530 20:26:46.422337    6964 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0530 20:26:46.422337    6964 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0530 20:26:46.422337    6964 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0530 20:26:46.422865    6964 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0530 20:26:46.422865    6964 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0530 20:26:46.422865    6964 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0530 20:26:46.424716    6964 out.go:235]     ▪ Booting up control plane ...
I0530 20:26:46.425408    6964 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0530 20:26:46.425408    6964 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0530 20:26:46.425408    6964 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0530 20:26:46.425966    6964 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0530 20:26:46.425966    6964 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0530 20:26:46.425966    6964 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0530 20:26:46.425966    6964 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0530 20:26:46.425966    6964 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0530 20:26:46.425966    6964 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 1.000987542s
I0530 20:26:46.426475    6964 kubeadm.go:310] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I0530 20:26:46.426475    6964 kubeadm.go:310] [control-plane-check] Checking kube-apiserver at https://192.168.49.2:8443/livez
I0530 20:26:46.426475    6964 kubeadm.go:310] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I0530 20:26:46.426475    6964 kubeadm.go:310] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I0530 20:26:46.426475    6964 kubeadm.go:310] [control-plane-check] kube-scheduler is healthy after 13.950902982s
I0530 20:26:46.426475    6964 kubeadm.go:310] [control-plane-check] kube-controller-manager is healthy after 14.202380765s
I0530 20:26:46.426475    6964 kubeadm.go:310] [control-plane-check] kube-apiserver is healthy after 15.503503817s
I0530 20:26:46.427012    6964 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0530 20:26:46.427012    6964 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0530 20:26:46.427012    6964 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0530 20:26:46.427543    6964 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0530 20:26:46.427543    6964 kubeadm.go:310] [bootstrap-token] Using token: 0yjd1f.km32ma5ojauspn6g
I0530 20:26:46.429133    6964 out.go:235]     ▪ Configuring RBAC rules ...
I0530 20:26:46.429133    6964 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0530 20:26:46.429133    6964 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0530 20:26:46.429663    6964 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0530 20:26:46.429663    6964 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0530 20:26:46.429663    6964 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0530 20:26:46.429663    6964 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0530 20:26:46.430196    6964 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0530 20:26:46.430196    6964 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0530 20:26:46.430196    6964 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0530 20:26:46.430196    6964 kubeadm.go:310] 
I0530 20:26:46.430196    6964 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0530 20:26:46.430196    6964 kubeadm.go:310] 
I0530 20:26:46.430732    6964 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0530 20:26:46.430732    6964 kubeadm.go:310] 
I0530 20:26:46.430732    6964 kubeadm.go:310]   mkdir -p $HOME/.kube
I0530 20:26:46.430732    6964 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0530 20:26:46.430732    6964 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0530 20:26:46.430732    6964 kubeadm.go:310] 
I0530 20:26:46.430732    6964 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0530 20:26:46.430732    6964 kubeadm.go:310] 
I0530 20:26:46.430732    6964 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0530 20:26:46.430732    6964 kubeadm.go:310] 
I0530 20:26:46.430732    6964 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0530 20:26:46.431266    6964 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0530 20:26:46.431266    6964 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0530 20:26:46.431266    6964 kubeadm.go:310] 
I0530 20:26:46.431266    6964 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0530 20:26:46.431266    6964 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0530 20:26:46.431266    6964 kubeadm.go:310] 
I0530 20:26:46.431797    6964 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token 0yjd1f.km32ma5ojauspn6g \
I0530 20:26:46.431797    6964 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:12d1771a215a59923a2c0ad82a35d2534adebe211a32cd39632684be5213a147 \
I0530 20:26:46.431797    6964 kubeadm.go:310] 	--control-plane 
I0530 20:26:46.431797    6964 kubeadm.go:310] 
I0530 20:26:46.431797    6964 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0530 20:26:46.431797    6964 kubeadm.go:310] 
I0530 20:26:46.432324    6964 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token 0yjd1f.km32ma5ojauspn6g \
I0530 20:26:46.432324    6964 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:12d1771a215a59923a2c0ad82a35d2534adebe211a32cd39632684be5213a147 
I0530 20:26:46.432324    6964 cni.go:84] Creating CNI manager for ""
I0530 20:26:46.432324    6964 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0530 20:26:46.433910    6964 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0530 20:26:46.445158    6964 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0530 20:26:46.476897    6964 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0530 20:26:46.512985    6964 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0530 20:26:46.525374    6964 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_05_30T20_26_46_0700 minikube.k8s.io/version=v1.36.0 minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0530 20:26:46.525905    6964 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0530 20:26:46.530156    6964 ops.go:34] apiserver oom_adj: -16
I0530 20:26:46.663048    6964 kubeadm.go:1105] duration metric: took 148.4669ms to wait for elevateKubeSystemPrivileges
I0530 20:26:46.663048    6964 kubeadm.go:394] duration metric: took 21.606653s to StartCluster
I0530 20:26:46.663048    6964 settings.go:142] acquiring lock: {Name:mk2e6e3bf7e50b8b2cd14e229cd91c361e0b98b1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0530 20:26:46.663048    6964 settings.go:150] Updating kubeconfig:  C:\Users\prati\.kube\config
I0530 20:26:46.666736    6964 lock.go:35] WriteFile acquiring C:\Users\prati\.kube\config: {Name:mk549fb056fba4ca9945cef78253d164e20c237e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0530 20:26:46.667791    6964 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0530 20:26:46.667791    6964 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0530 20:26:46.667791    6964 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0530 20:26:46.668924    6964 out.go:177] 🔎  Verifying Kubernetes components...
I0530 20:26:46.670921    6964 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0530 20:26:46.675165    6964 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0530 20:26:46.675165    6964 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0530 20:26:46.676752    6964 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0530 20:26:46.676752    6964 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0530 20:26:46.677281    6964 host.go:66] Checking if "minikube" exists ...
I0530 20:26:46.677815    6964 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0530 20:26:46.686691    6964 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0530 20:26:46.687780    6964 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0530 20:26:46.774513    6964 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0530 20:26:46.776138    6964 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0530 20:26:46.776138    6964 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0530 20:26:46.781382    6964 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0530 20:26:46.808692    6964 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0530 20:26:46.808692    6964 host.go:66] Checking if "minikube" exists ...
I0530 20:26:46.815786    6964 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0530 20:26:46.834247    6964 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0530 20:26:46.841703    6964 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51206 SSHKeyPath:C:\Users\prati\.minikube\machines\minikube\id_rsa Username:docker}
I0530 20:26:46.879066    6964 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0530 20:26:46.879066    6964 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0530 20:26:46.882282    6964 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0530 20:26:46.925310    6964 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51206 SSHKeyPath:C:\Users\prati\.minikube\machines\minikube\id_rsa Username:docker}
I0530 20:26:47.033558    6964 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0530 20:26:47.138903    6964 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0530 20:26:47.161871    6964 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0530 20:26:47.558504    6964 start.go:971] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I0530 20:26:47.563561    6964 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0530 20:26:47.610787    6964 api_server.go:52] waiting for apiserver process to appear ...
I0530 20:26:47.615038    6964 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0530 20:26:47.936426    6964 api_server.go:72] duration metric: took 1.2686344s to wait for apiserver process to appear ...
I0530 20:26:47.936426    6964 api_server.go:88] waiting for apiserver healthz status ...
I0530 20:26:47.936937    6964 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51210/healthz ...
I0530 20:26:47.951806    6964 api_server.go:279] https://127.0.0.1:51210/healthz returned 200:
ok
I0530 20:26:47.957826    6964 api_server.go:141] control plane version: v1.33.1
I0530 20:26:47.958372    6964 api_server.go:131] duration metric: took 21.9467ms to wait for apiserver health ...
I0530 20:26:47.958372    6964 system_pods.go:43] waiting for kube-system pods to appear ...
I0530 20:26:47.963284    6964 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0530 20:26:47.965894    6964 addons.go:514] duration metric: took 1.2986327s for enable addons: enabled=[storage-provisioner default-storageclass]
I0530 20:26:47.975048    6964 system_pods.go:59] 5 kube-system pods found
I0530 20:26:47.975048    6964 system_pods.go:61] "etcd-minikube" [b08a464d-d62a-4d39-9852-1788d10c9531] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0530 20:26:47.975048    6964 system_pods.go:61] "kube-apiserver-minikube" [9d6bfb14-c5b5-48c8-bc16-99698de968fe] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0530 20:26:47.975048    6964 system_pods.go:61] "kube-controller-manager-minikube" [5766f97c-7c16-44b1-8ff4-67dd99b867e4] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0530 20:26:47.975048    6964 system_pods.go:61] "kube-scheduler-minikube" [d022f6e6-9caa-4370-a45d-798dbed97fca] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0530 20:26:47.975048    6964 system_pods.go:61] "storage-provisioner" [fe05da76-4266-42a2-b906-d2fffcc46040] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.)
I0530 20:26:47.975048    6964 system_pods.go:74] duration metric: took 16.6758ms to wait for pod list to return data ...
I0530 20:26:47.975048    6964 kubeadm.go:578] duration metric: took 1.3072569s to wait for: map[apiserver:true system_pods:true]
I0530 20:26:47.975048    6964 node_conditions.go:102] verifying NodePressure condition ...
I0530 20:26:47.979926    6964 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0530 20:26:47.979926    6964 node_conditions.go:123] node cpu capacity is 8
I0530 20:26:47.980952    6964 node_conditions.go:105] duration metric: took 5.3814ms to run NodePressure ...
I0530 20:26:47.980952    6964 start.go:241] waiting for startup goroutines ...
I0530 20:26:48.065653    6964 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0530 20:26:48.065653    6964 start.go:246] waiting for cluster config update ...
I0530 20:26:48.065653    6964 start.go:255] writing updated cluster config ...
I0530 20:26:48.067247    6964 ssh_runner.go:195] Run: rm -f paused
I0530 20:26:48.077918    6964 out.go:177] 💡  kubectl not found. If you need it, try: 'minikube kubectl -- get pods -A'
I0530 20:26:48.079624    6964 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
May 30 14:56:18 minikube dockerd[1165]: time="2025-05-30T14:56:18.158892096Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
May 30 14:56:18 minikube dockerd[1165]: time="2025-05-30T14:56:18.158900396Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
May 30 14:56:18 minikube dockerd[1165]: time="2025-05-30T14:56:18.158904796Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
May 30 14:56:18 minikube dockerd[1165]: time="2025-05-30T14:56:18.158924697Z" level=info msg="Docker daemon" commit=01f442b containerd-snapshotter=false storage-driver=overlay2 version=28.1.1
May 30 14:56:18 minikube dockerd[1165]: time="2025-05-30T14:56:18.158967899Z" level=info msg="Initializing buildkit"
May 30 14:56:18 minikube dockerd[1165]: time="2025-05-30T14:56:18.200059323Z" level=info msg="Completed buildkit initialization"
May 30 14:56:18 minikube dockerd[1165]: time="2025-05-30T14:56:18.205425061Z" level=info msg="Daemon has completed initialization"
May 30 14:56:18 minikube dockerd[1165]: time="2025-05-30T14:56:18.205529766Z" level=info msg="API listen on [::]:2376"
May 30 14:56:18 minikube dockerd[1165]: time="2025-05-30T14:56:18.205573168Z" level=info msg="API listen on /var/run/docker.sock"
May 30 14:56:18 minikube dockerd[1165]: time="2025-05-30T14:56:18.206829924Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
May 30 14:56:18 minikube dockerd[1165]: time="2025-05-30T14:56:18.207614758Z" level=info msg="Daemon shutdown complete"
May 30 14:56:18 minikube systemd[1]: docker.service: Deactivated successfully.
May 30 14:56:18 minikube systemd[1]: Stopped Docker Application Container Engine.
May 30 14:56:18 minikube systemd[1]: Starting Docker Application Container Engine...
May 30 14:56:18 minikube dockerd[1471]: time="2025-05-30T14:56:18.249543319Z" level=info msg="Starting up"
May 30 14:56:18 minikube dockerd[1471]: time="2025-05-30T14:56:18.250725972Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
May 30 14:56:18 minikube dockerd[1471]: time="2025-05-30T14:56:18.261785363Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
May 30 14:56:18 minikube dockerd[1471]: time="2025-05-30T14:56:18.276766228Z" level=info msg="[graphdriver] trying configured driver: overlay2"
May 30 14:56:18 minikube dockerd[1471]: time="2025-05-30T14:56:18.340707165Z" level=info msg="Loading containers: start."
May 30 14:56:21 minikube dockerd[1471]: time="2025-05-30T14:56:21.076940507Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count d3f515f2ae1724e94e0ec53a05dd18573dd04968fbe5653ba2c5fb6b03de83e0], retrying...."
May 30 14:56:21 minikube dockerd[1471]: time="2025-05-30T14:56:21.127332044Z" level=info msg="Loading containers: done."
May 30 14:56:21 minikube dockerd[1471]: time="2025-05-30T14:56:21.188305450Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
May 30 14:56:21 minikube dockerd[1471]: time="2025-05-30T14:56:21.188356152Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
May 30 14:56:21 minikube dockerd[1471]: time="2025-05-30T14:56:21.188364953Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
May 30 14:56:21 minikube dockerd[1471]: time="2025-05-30T14:56:21.188369953Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
May 30 14:56:21 minikube dockerd[1471]: time="2025-05-30T14:56:21.188390654Z" level=info msg="Docker daemon" commit=01f442b containerd-snapshotter=false storage-driver=overlay2 version=28.1.1
May 30 14:56:21 minikube dockerd[1471]: time="2025-05-30T14:56:21.188441656Z" level=info msg="Initializing buildkit"
May 30 14:56:21 minikube dockerd[1471]: time="2025-05-30T14:56:21.238983799Z" level=info msg="Completed buildkit initialization"
May 30 14:56:21 minikube dockerd[1471]: time="2025-05-30T14:56:21.248086803Z" level=info msg="Daemon has completed initialization"
May 30 14:56:21 minikube dockerd[1471]: time="2025-05-30T14:56:21.248266411Z" level=info msg="API listen on /var/run/docker.sock"
May 30 14:56:21 minikube dockerd[1471]: time="2025-05-30T14:56:21.248353415Z" level=info msg="API listen on [::]:2376"
May 30 14:56:21 minikube systemd[1]: Started Docker Application Container Engine.
May 30 14:56:21 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
May 30 14:56:22 minikube cri-dockerd[1780]: time="2025-05-30T14:56:22Z" level=info msg="Starting cri-dockerd dev (HEAD)"
May 30 14:56:22 minikube cri-dockerd[1780]: time="2025-05-30T14:56:22Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
May 30 14:56:22 minikube cri-dockerd[1780]: time="2025-05-30T14:56:22Z" level=info msg="Start docker client with request timeout 0s"
May 30 14:56:22 minikube cri-dockerd[1780]: time="2025-05-30T14:56:22Z" level=info msg="Hairpin mode is set to hairpin-veth"
May 30 14:56:22 minikube cri-dockerd[1780]: time="2025-05-30T14:56:22Z" level=info msg="Loaded network plugin cni"
May 30 14:56:22 minikube cri-dockerd[1780]: time="2025-05-30T14:56:22Z" level=info msg="Docker cri networking managed by network plugin cni"
May 30 14:56:22 minikube cri-dockerd[1780]: time="2025-05-30T14:56:22Z" level=info msg="Setting cgroupDriver cgroupfs"
May 30 14:56:22 minikube cri-dockerd[1780]: time="2025-05-30T14:56:22Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
May 30 14:56:22 minikube cri-dockerd[1780]: time="2025-05-30T14:56:22Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
May 30 14:56:22 minikube cri-dockerd[1780]: time="2025-05-30T14:56:22Z" level=info msg="Start cri-dockerd grpc backend"
May 30 14:56:22 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
May 30 14:56:38 minikube cri-dockerd[1780]: time="2025-05-30T14:56:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d39f5cd3bdf369d0cb81dfe1d0cd7015a01530e13c2c3587ec7d42aa041e1d55/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 30 14:56:38 minikube cri-dockerd[1780]: time="2025-05-30T14:56:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/80a8de9378a21f9bac5addcbf06f34f73524e4e88073729ed3c38c4870a4f4f2/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 30 14:56:38 minikube cri-dockerd[1780]: time="2025-05-30T14:56:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/af51ef646e0b5488063dd39129dec2ba28b56807da828c01c75282e55753f26a/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 30 14:56:38 minikube cri-dockerd[1780]: time="2025-05-30T14:56:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f3dbd99c23770fc1c8452baab5ea02d017af4b5de3a18448e70b74f2ff47b330/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 30 14:56:50 minikube cri-dockerd[1780]: time="2025-05-30T14:56:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b1c5f5c24604abb113a2b0dd224de90ab3acea3ac8baaf7056b2d19bce7619df/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 30 14:56:50 minikube cri-dockerd[1780]: time="2025-05-30T14:56:50Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5fa1d2e5a293b955adb4a1a35a407f31c890e8404255410eb39654b94463596f/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 30 14:56:51 minikube cri-dockerd[1780]: time="2025-05-30T14:56:51Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/727d90f526c4cab60ac314eeaf9fc720bbe2c6af3a2fad7a2134ab3cc255fef6/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 30 14:56:56 minikube cri-dockerd[1780]: time="2025-05-30T14:56:56Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
May 30 14:57:12 minikube dockerd[1471]: time="2025-05-30T14:57:12.442668079Z" level=info msg="ignoring event" container=3e6da2a0a970261f5f509e50ecf6d6f4dea2688bef17130b27314e739c2affab module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 30 14:58:02 minikube cri-dockerd[1780]: time="2025-05-30T14:58:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/dabd76bdf7fc68499e5fd10ae13510539b427ce55b96e20030065e9746bd106a/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 30 14:58:05 minikube dockerd[1471]: time="2025-05-30T14:58:05.449756695Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 30 14:58:05 minikube dockerd[1471]: time="2025-05-30T14:58:05.449842098Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 30 14:58:22 minikube dockerd[1471]: time="2025-05-30T14:58:22.050625193Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 30 14:58:22 minikube dockerd[1471]: time="2025-05-30T14:58:22.050836802Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 30 14:58:52 minikube dockerd[1471]: time="2025-05-30T14:58:52.783144501Z" level=error msg="Not continuing with pull after error" error="errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 30 14:58:52 minikube dockerd[1471]: time="2025-05-30T14:58:52.783227105Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
47cce16639651       6e38f40d628db       About a minute ago   Running             storage-provisioner       1                   b1c5f5c24604a       storage-provisioner
979a919457ae0       1cf5f116067c6       2 minutes ago        Running             coredns                   0                   727d90f526c4c       coredns-674b8bbfcf-b92tm
40f801b9e3720       b79c189b052cd       2 minutes ago        Running             kube-proxy                0                   5fa1d2e5a293b       kube-proxy-nqbxq
3e6da2a0a9702       6e38f40d628db       2 minutes ago        Exited              storage-provisioner       0                   b1c5f5c24604a       storage-provisioner
5d38aa9e97e3b       499038711c081       2 minutes ago        Running             etcd                      0                   f3dbd99c23770       etcd-minikube
0183b83fd20dc       c6ab243b29f82       2 minutes ago        Running             kube-apiserver            0                   af51ef646e0b5       kube-apiserver-minikube
b2b7ffc75a1e4       398c985c0d950       2 minutes ago        Running             kube-scheduler            0                   d39f5cd3bdf36       kube-scheduler-minikube
51b6cedec466b       ef43894fa110c       2 minutes ago        Running             kube-controller-manager   0                   80a8de9378a21       kube-controller-manager-minikube


==> coredns [979a919457ae] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] 127.0.0.1:59291 - 18576 "HINFO IN 1033535655722622559.5481229106744036705. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.197739044s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_05_30T20_26_46_0700
                    minikube.k8s.io/version=v1.36.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 30 May 2025 14:56:42 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 30 May 2025 14:58:49 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 30 May 2025 14:56:56 +0000   Fri, 30 May 2025 14:56:41 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 30 May 2025 14:56:56 +0000   Fri, 30 May 2025 14:56:41 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 30 May 2025 14:56:56 +0000   Fri, 30 May 2025 14:56:41 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 30 May 2025 14:56:56 +0000   Fri, 30 May 2025 14:56:45 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3934232Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3934232Ki
  pods:               110
System Info:
  Machine ID:                 e8881be4ac234fbbb1b106a3e53245a3
  System UUID:                e8881be4ac234fbbb1b106a3e53245a3
  Boot ID:                    3cb1d8c5-0e35-4823-8698-38bc7a9e10c9
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                    ------------  ----------  ---------------  -------------  ---
  default                     flaskapp-deployment-55d46dc985-kv4ws    0 (0%)        0 (0%)      0 (0%)           0 (0%)         55s
  kube-system                 coredns-674b8bbfcf-b92tm                100m (1%)     0 (0%)      70Mi (1%)        170Mi (4%)     2m6s
  kube-system                 etcd-minikube                           100m (1%)     0 (0%)      100Mi (2%)       0 (0%)         2m11s
  kube-system                 kube-apiserver-minikube                 250m (3%)     0 (0%)      0 (0%)           0 (0%)         2m11s
  kube-system                 kube-controller-manager-minikube        200m (2%)     0 (0%)      0 (0%)           0 (0%)         2m11s
  kube-system                 kube-proxy-nqbxq                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m6s
  kube-system                 kube-scheduler-minikube                 100m (1%)     0 (0%)      0 (0%)           0 (0%)         2m11s
  kube-system                 storage-provisioner                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m9s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%)   0 (0%)
  memory             170Mi (4%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                    From             Message
  ----     ------                             ----                   ----             -------
  Normal   Starting                           2m5s                   kube-proxy       
  Normal   NodeAllocatableEnforced            2m27s                  kubelet          Updated Node Allocatable limit across pods
  Warning  CgroupV1                           2m27s                  kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeHasSufficientMemory            2m27s (x8 over 2m27s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              2m27s (x8 over 2m27s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               2m27s (x7 over 2m27s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   Starting                           2m27s                  kubelet          Starting kubelet.
  Normal   Starting                           2m11s                  kubelet          Starting kubelet.
  Warning  PossibleMemoryBackedVolumesOnDisk  2m11s                  kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Warning  CgroupV1                           2m11s                  kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeAllocatableEnforced            2m10s                  kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            2m10s                  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              2m10s                  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               2m10s                  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode                     2m7s                   node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.000822] PCI: Fatal: No config space access function found
[  +0.024354] PCI: System does not support PCI
[  +0.155371] kvm: already loaded the other module
[  +2.738859] FS-Cache: Duplicate cookie detected
[  +0.008336] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.012889] FS-Cache: O-cookie d=00000000ce864667{9P.session} n=00000000f5a205b1
[  +0.003040] FS-Cache: O-key=[10] '34323934393337353839'
[  +0.000490] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000567] FS-Cache: N-cookie d=00000000ce864667{9P.session} n=00000000c49be0b9
[  +0.000874] FS-Cache: N-key=[10] '34323934393337353839'
[  +0.326540] FS-Cache: Duplicate cookie detected
[  +0.000446] FS-Cache: O-cookie c=00000008 [p=00000007 fl=226 nc=0 na=1]
[  +0.000491] FS-Cache: O-cookie d=000000002456a227{9p.inode} n=000000004772e47a
[  +0.000539] FS-Cache: O-key=[8] '3385050000000500'
[  +0.000381] FS-Cache: N-cookie c=00000009 [p=00000007 fl=2 nc=0 na=1]
[  +0.000482] FS-Cache: N-cookie d=000000002456a227{9p.inode} n=0000000037b40b0e
[  +0.000778] FS-Cache: N-key=[8] '3385050000000500'
[  +0.001466] FS-Cache: Duplicate cookie detected
[  +0.000450] FS-Cache: O-cookie c=00000008 [p=00000007 fl=226 nc=0 na=1]
[  +0.000445] FS-Cache: O-cookie d=000000002456a227{9p.inode} n=000000004772e47a
[  +0.000653] FS-Cache: O-key=[8] '3385050000000500'
[  +0.000412] FS-Cache: N-cookie c=0000000a [p=00000007 fl=2 nc=0 na=1]
[  +0.000484] FS-Cache: N-cookie d=000000002456a227{9p.inode} n=00000000d9d88a48
[  +0.000552] FS-Cache: N-key=[8] '3385050000000500'
[  +0.168916] FS-Cache: Duplicate cookie detected
[  +0.000632] FS-Cache: O-cookie c=0000000c [p=00000007 fl=226 nc=0 na=1]
[  +0.000542] FS-Cache: O-cookie d=000000002456a227{9p.inode} n=0000000025b60191
[  +0.000643] FS-Cache: O-key=[8] '6c85050000000500'
[  +0.000427] FS-Cache: N-cookie c=0000000d [p=00000007 fl=2 nc=0 na=1]
[  +0.000545] FS-Cache: N-cookie d=000000002456a227{9p.inode} n=000000007aa8133d
[  +0.000704] FS-Cache: N-key=[8] '6c85050000000500'
[  +0.072878] FS-Cache: Duplicate cookie detected
[  +0.000486] FS-Cache: O-cookie c=0000000f [p=00000007 fl=226 nc=0 na=1]
[  +0.000507] FS-Cache: O-cookie d=000000002456a227{9p.inode} n=0000000073e3c294
[  +0.000593] FS-Cache: O-key=[8] '7385050000000500'
[  +0.000405] FS-Cache: N-cookie c=00000010 [p=00000007 fl=2 nc=0 na=1]
[  +0.000485] FS-Cache: N-cookie d=000000002456a227{9p.inode} n=000000001033d775
[  +0.000558] FS-Cache: N-key=[8] '7385050000000500'
[  +0.045447] FS-Cache: Duplicate cookie detected
[  +0.000631] FS-Cache: O-cookie c=00000012 [p=00000007 fl=226 nc=0 na=1]
[  +0.000514] FS-Cache: O-cookie d=000000002456a227{9p.inode} n=000000003ccfaee5
[  +0.000624] FS-Cache: O-key=[8] 'f884050000000500'
[  +0.000399] FS-Cache: N-cookie c=00000013 [p=00000007 fl=2 nc=0 na=1]
[  +0.000494] FS-Cache: N-cookie d=000000002456a227{9p.inode} n=00000000f9863119
[  +0.000620] FS-Cache: N-key=[8] 'f884050000000500'
[  +0.449640] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2542: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.026388] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.518536] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.020468] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001907] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001663] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000901] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002039] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001065] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001435] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.003066] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +2.794592] netlink: 'init': attribute type 4 has an invalid length.
[  +0.058614] WSL (151) ERROR: CheckConnection: getaddrinfo() failed: -5
[May30 14:56] tmpfs: Unknown parameter 'noswap'
[ +16.706645] tmpfs: Unknown parameter 'noswap'


==> etcd [5d38aa9e97e3] <==
{"level":"info","ts":"2025-05-30T14:56:39.342063Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-05-30T14:56:39.346327Z","caller":"etcdserver/server.go:875","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.21","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-05-30T14:56:39.347150Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-05-30T14:56:39.347234Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-05-30T14:56:39.347356Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-05-30T14:56:39.347312Z","caller":"etcdserver/server.go:759","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-05-30T14:56:39.347378Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-05-30T14:56:39.349952Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-05-30T14:56:39.350779Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"],"added-peer-is-learner":false}
{"level":"info","ts":"2025-05-30T14:56:39.354665Z","caller":"embed/etcd.go:762","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-05-30T14:56:39.354806Z","caller":"embed/etcd.go:633","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-30T14:56:39.354856Z","caller":"embed/etcd.go:603","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-30T14:56:39.355146Z","caller":"embed/etcd.go:292","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-05-30T14:56:39.355232Z","caller":"embed/etcd.go:908","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-05-30T14:56:39.721780Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2025-05-30T14:56:39.721873Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2025-05-30T14:56:39.721904Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2025-05-30T14:56:39.721949Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2025-05-30T14:56:39.722006Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-05-30T14:56:39.722021Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2025-05-30T14:56:39.722031Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-05-30T14:56:39.723958Z","caller":"etcdserver/server.go:2697","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-05-30T14:56:39.725709Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-05-30T14:56:39.725753Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-05-30T14:56:39.725700Z","caller":"etcdserver/server.go:2144","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-05-30T14:56:39.726462Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-05-30T14:56:39.726687Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-05-30T14:56:39.726601Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-05-30T14:56:39.727551Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-05-30T14:56:39.728948Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-05-30T14:56:39.729646Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-05-30T14:56:39.732226Z","caller":"membership/cluster.go:587","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-05-30T14:56:39.732439Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-05-30T14:56:39.732548Z","caller":"etcdserver/server.go:2721","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"warn","ts":"2025-05-30T14:57:06.967808Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"268.180165ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-30T14:57:06.968106Z","caller":"traceutil/trace.go:171","msg":"trace[2073563956] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:427; }","duration":"268.485177ms","start":"2025-05-30T14:57:06.699591Z","end":"2025-05-30T14:57:06.968076Z","steps":["trace[2073563956] 'range keys from in-memory index tree'  (duration: 268.168364ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-30T14:57:06.969234Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"278.901507ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128037598472004586 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/kube-system/coredns-674b8bbfcf-b92tm.184455e9776cac45\" mod_revision:422 > success:<request_put:<key:\"/registry/events/kube-system/coredns-674b8bbfcf-b92tm.184455e9776cac45\" value_size:674 lease:8128037598472003745 >> failure:<request_range:<key:\"/registry/events/kube-system/coredns-674b8bbfcf-b92tm.184455e9776cac45\" > >>","response":"size:16"}
{"level":"info","ts":"2025-05-30T14:57:06.969415Z","caller":"traceutil/trace.go:171","msg":"trace[1528051263] transaction","detail":"{read_only:false; response_revision:428; number_of_response:1; }","duration":"361.334308ms","start":"2025-05-30T14:57:06.608066Z","end":"2025-05-30T14:57:06.969400Z","steps":["trace[1528051263] 'process raft request'  (duration: 81.102846ms)","trace[1528051263] 'compare'  (duration: 278.699699ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-30T14:57:06.969515Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-30T14:57:06.608054Z","time spent":"361.402311ms","remote":"127.0.0.1:42300","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":762,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/events/kube-system/coredns-674b8bbfcf-b92tm.184455e9776cac45\" mod_revision:422 > success:<request_put:<key:\"/registry/events/kube-system/coredns-674b8bbfcf-b92tm.184455e9776cac45\" value_size:674 lease:8128037598472003745 >> failure:<request_range:<key:\"/registry/events/kube-system/coredns-674b8bbfcf-b92tm.184455e9776cac45\" > >"}
{"level":"warn","ts":"2025-05-30T14:57:10.313006Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"255.673804ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128037598472004596 > lease_revoke:<id:70cc9721b1f163b3>","response":"size:29"}
{"level":"info","ts":"2025-05-30T14:57:16.708208Z","caller":"traceutil/trace.go:171","msg":"trace[1357496509] transaction","detail":"{read_only:false; response_revision:439; number_of_response:1; }","duration":"154.594212ms","start":"2025-05-30T14:57:16.553575Z","end":"2025-05-30T14:57:16.708169Z","steps":["trace[1357496509] 'process raft request'  (duration: 55.712211ms)","trace[1357496509] 'compare'  (duration: 98.536787ms)"],"step_count":2}
{"level":"info","ts":"2025-05-30T14:57:16.915352Z","caller":"traceutil/trace.go:171","msg":"trace[380641404] transaction","detail":"{read_only:false; response_revision:442; number_of_response:1; }","duration":"200.449414ms","start":"2025-05-30T14:57:16.714883Z","end":"2025-05-30T14:57:16.915333Z","steps":["trace[380641404] 'process raft request'  (duration: 114.441647ms)","trace[380641404] 'compare'  (duration: 85.861061ms)"],"step_count":2}
{"level":"info","ts":"2025-05-30T14:58:49.126610Z","caller":"traceutil/trace.go:171","msg":"trace[11806653] transaction","detail":"{read_only:false; response_revision:551; number_of_response:1; }","duration":"191.281426ms","start":"2025-05-30T14:58:48.935238Z","end":"2025-05-30T14:58:49.126519Z","steps":["trace[11806653] 'process raft request'  (duration: 190.229781ms)"],"step_count":1}
{"level":"info","ts":"2025-05-30T14:58:49.771431Z","caller":"traceutil/trace.go:171","msg":"trace[803922426] transaction","detail":"{read_only:false; response_revision:552; number_of_response:1; }","duration":"832.879019ms","start":"2025-05-30T14:58:48.938540Z","end":"2025-05-30T14:58:49.771419Z","steps":["trace[803922426] 'process raft request'  (duration: 832.753513ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-30T14:58:49.771575Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-30T14:58:48.938523Z","time spent":"832.979723ms","remote":"127.0.0.1:42452","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":3170,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/pods/default/flaskapp-deployment-55d46dc985-kv4ws\" mod_revision:538 > success:<request_put:<key:\"/registry/pods/default/flaskapp-deployment-55d46dc985-kv4ws\" value_size:3103 >> failure:<request_range:<key:\"/registry/pods/default/flaskapp-deployment-55d46dc985-kv4ws\" > >"}
{"level":"warn","ts":"2025-05-30T14:58:50.391615Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"495.592713ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128037598472005159 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:543 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:471 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >>","response":"size:16"}
{"level":"info","ts":"2025-05-30T14:58:50.391762Z","caller":"traceutil/trace.go:171","msg":"trace[1687826004] transaction","detail":"{read_only:false; response_revision:553; number_of_response:1; }","duration":"1.187709478s","start":"2025-05-30T14:58:49.204035Z","end":"2025-05-30T14:58:50.391745Z","steps":["trace[1687826004] 'process raft request'  (duration: 691.932957ms)","trace[1687826004] 'compare'  (duration: 495.365304ms)"],"step_count":2}
{"level":"info","ts":"2025-05-30T14:58:50.391807Z","caller":"traceutil/trace.go:171","msg":"trace[1426042880] linearizableReadLoop","detail":"{readStateIndex:594; appliedIndex:592; }","duration":"1.118288393s","start":"2025-05-30T14:58:49.273507Z","end":"2025-05-30T14:58:50.391795Z","steps":["trace[1426042880] 'read index received'  (duration: 497.722905ms)","trace[1426042880] 'applied index is now lower than readState.Index'  (duration: 620.564488ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-30T14:58:50.391857Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-30T14:58:49.204014Z","time spent":"1.187789682s","remote":"127.0.0.1:42534","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":520,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:543 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:471 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2025-05-30T14:58:50.794693Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"402.939828ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128037598472005160 > lease_revoke:<id:70cc9721b1f165d0>","response":"size:29"}
{"level":"info","ts":"2025-05-30T14:58:50.794869Z","caller":"traceutil/trace.go:171","msg":"trace[662132337] linearizableReadLoop","detail":"{readStateIndex:595; appliedIndex:594; }","duration":"403.010332ms","start":"2025-05-30T14:58:50.391839Z","end":"2025-05-30T14:58:50.794849Z","steps":["trace[662132337] 'read index received'  (duration: 55.603µs)","trace[662132337] 'applied index is now lower than readState.Index'  (duration: 402.953329ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-30T14:58:50.794942Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.099986105s","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-05-30T14:58:50.794955Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.52143853s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:600"}
{"level":"info","ts":"2025-05-30T14:58:50.794966Z","caller":"traceutil/trace.go:171","msg":"trace[880886432] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:553; }","duration":"1.100018407s","start":"2025-05-30T14:58:49.694937Z","end":"2025-05-30T14:58:50.794956Z","steps":["trace[880886432] 'agreement among raft nodes before linearized reading'  (duration: 1.099966205s)"],"step_count":1}
{"level":"info","ts":"2025-05-30T14:58:50.794981Z","caller":"traceutil/trace.go:171","msg":"trace[630572700] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:553; }","duration":"1.521470933s","start":"2025-05-30T14:58:49.273502Z","end":"2025-05-30T14:58:50.794973Z","steps":["trace[630572700] 'agreement among raft nodes before linearized reading'  (duration: 1.118379897s)","trace[630572700] 'range keys from in-memory index tree'  (duration: 403.005031ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-30T14:58:50.795014Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-30T14:58:49.273457Z","time spent":"1.521547435s","remote":"127.0.0.1:42424","response type":"/etcdserverpb.KV/Range","request count":0,"request size":69,"response count":1,"response size":624,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 "}
{"level":"warn","ts":"2025-05-30T14:58:50.795038Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"664.074659ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-30T14:58:50.795078Z","caller":"traceutil/trace.go:171","msg":"trace[527205130] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:553; }","duration":"664.126761ms","start":"2025-05-30T14:58:50.130938Z","end":"2025-05-30T14:58:50.795065Z","steps":["trace[527205130] 'agreement among raft nodes before linearized reading'  (duration: 664.056158ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-30T14:58:50.795109Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-30T14:58:50.130924Z","time spent":"664.179363ms","remote":"127.0.0.1:42226","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-05-30T14:58:50.901319Z","caller":"traceutil/trace.go:171","msg":"trace[472642267] transaction","detail":"{read_only:false; response_revision:554; number_of_response:1; }","duration":"101.818879ms","start":"2025-05-30T14:58:50.799489Z","end":"2025-05-30T14:58:50.901308Z","steps":["trace[472642267] 'process raft request'  (duration: 101.718774ms)"],"step_count":1}


==> kernel <==
 14:58:57 up 13 min,  0 users,  load average: 0.51, 0.33, 0.13
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [0183b83fd20d] <==
I0530 14:56:42.756400       1 default_servicecidr_controller.go:110] Starting kubernetes-service-cidr-controller
I0530 14:56:42.757090       1 shared_informer.go:350] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I0530 14:56:42.756732       1 repairip.go:200] Starting ipallocator-repair-controller
I0530 14:56:42.757340       1 shared_informer.go:350] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I0530 14:56:42.755718       1 controller.go:78] Starting OpenAPI AggregationController
I0530 14:56:42.755704       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0530 14:56:42.757888       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0530 14:56:42.756940       1 controller.go:142] Starting OpenAPI controller
I0530 14:56:42.756957       1 controller.go:90] Starting OpenAPI V3 controller
I0530 14:56:42.756965       1 naming_controller.go:299] Starting NamingConditionController
I0530 14:56:42.756976       1 establishing_controller.go:81] Starting EstablishingController
I0530 14:56:42.757012       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0530 14:56:42.757027       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0530 14:56:42.757034       1 crd_finalizer.go:269] Starting CRDFinalizer
I0530 14:56:42.923544       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0530 14:56:42.923854       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0530 14:56:42.931670       1 shared_informer.go:357] "Caches are synced" controller="node_authorizer"
I0530 14:56:42.931713       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0530 14:56:42.931743       1 shared_informer.go:357] "Caches are synced" controller="cluster_authentication_trust_controller"
I0530 14:56:42.931774       1 cache.go:39] Caches are synced for LocalAvailability controller
I0530 14:56:42.931813       1 shared_informer.go:357] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0530 14:56:42.931835       1 default_servicecidr_controller.go:165] Creating default ServiceCIDR with CIDRs: [10.96.0.0/12]
I0530 14:56:42.931874       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0530 14:56:42.931914       1 shared_informer.go:357] "Caches are synced" controller="ipallocator-repair-controller"
I0530 14:56:42.931959       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0530 14:56:42.956586       1 shared_informer.go:357] "Caches are synced" controller="configmaps"
I0530 14:56:42.956619       1 shared_informer.go:357] "Caches are synced" controller="crd-autoregister"
I0530 14:56:42.956807       1 aggregator.go:171] initial CRD sync complete...
I0530 14:56:42.956822       1 autoregister_controller.go:144] Starting autoregister controller
I0530 14:56:42.956830       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0530 14:56:42.956838       1 cache.go:39] Caches are synced for autoregister controller
I0530 14:56:43.022835       1 shared_informer.go:357] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0530 14:56:43.022884       1 policy_source.go:240] refreshing policies
E0530 14:56:43.024406       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
E0530 14:56:43.024533       1 controller.go:148] "Unhandled Error" err="while syncing ConfigMap \"kube-system/kube-apiserver-legacy-service-account-token-tracking\", err: namespaces \"kube-system\" not found" logger="UnhandledError"
I0530 14:56:43.031105       1 controller.go:667] quota admission added evaluator for: namespaces
I0530 14:56:43.038150       1 default_servicecidr_controller.go:214] Setting default ServiceCIDR condition Ready to True
I0530 14:56:43.038232       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0530 14:56:43.045949       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0530 14:56:43.046118       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0530 14:56:43.230350       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0530 14:56:43.762785       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0530 14:56:43.767153       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0530 14:56:43.767186       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0530 14:56:44.539690       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0530 14:56:44.585018       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0530 14:56:44.703496       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0530 14:56:44.745167       1 alloc.go:328] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0530 14:56:44.755759       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0530 14:56:44.757333       1 controller.go:667] quota admission added evaluator for: endpoints
I0530 14:56:44.766536       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0530 14:56:45.814255       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0530 14:56:45.834711       1 alloc.go:328] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0530 14:56:45.844489       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I0530 14:56:50.303727       1 controller.go:667] quota admission added evaluator for: controllerrevisions.apps
I0530 14:56:50.403295       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0530 14:56:50.532492       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0530 14:56:50.539966       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0530 14:58:01.770750       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0530 14:58:01.771890       1 alloc.go:328] "allocated clusterIPs" service="default/flaskapp-service" clusterIPs={"IPv4":"10.101.147.196"}


==> kube-controller-manager [51b6cedec466] <==
I0530 14:56:49.152709       1 controllermanager.go:778] "Started controller" controller="persistentvolume-protection-controller"
I0530 14:56:49.152785       1 pv_protection_controller.go:81] "Starting PV protection controller" logger="persistentvolume-protection-controller"
I0530 14:56:49.152799       1 shared_informer.go:350] "Waiting for caches to sync" controller="PV protection"
I0530 14:56:49.304371       1 controllermanager.go:778] "Started controller" controller="ephemeral-volume-controller"
I0530 14:56:49.304440       1 controller.go:173] "Starting ephemeral volume controller" logger="ephemeral-volume-controller"
I0530 14:56:49.304461       1 shared_informer.go:350] "Waiting for caches to sync" controller="ephemeral"
I0530 14:56:49.308905       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0530 14:56:49.321698       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0530 14:56:49.328411       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0530 14:56:49.328410       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0530 14:56:49.329781       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0530 14:56:49.329928       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0530 14:56:49.329938       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0530 14:56:49.350928       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0530 14:56:49.351530       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0530 14:56:49.352278       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0530 14:56:49.352415       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0530 14:56:49.353459       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0530 14:56:49.353596       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0530 14:56:49.354593       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0530 14:56:49.354671       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0530 14:56:49.357293       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0530 14:56:49.357468       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0530 14:56:49.357503       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0530 14:56:49.358799       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0530 14:56:49.376347       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0530 14:56:49.405793       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0530 14:56:49.409231       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0530 14:56:49.409612       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0530 14:56:49.411569       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0530 14:56:49.418157       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0530 14:56:49.425482       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0530 14:56:49.434529       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0530 14:56:49.434696       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0530 14:56:49.434823       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0530 14:56:49.434918       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0530 14:56:49.453478       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0530 14:56:49.456066       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0530 14:56:49.459933       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0530 14:56:49.503103       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0530 14:56:49.505763       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0530 14:56:49.506975       1 shared_informer.go:357] "Caches are synced" controller="node"
I0530 14:56:49.507032       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0530 14:56:49.507050       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0530 14:56:49.507057       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0530 14:56:49.507063       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0530 14:56:49.514413       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0530 14:56:49.540177       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0530 14:56:49.569199       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0530 14:56:49.603135       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0530 14:56:49.605964       1 shared_informer.go:357] "Caches are synced" controller="job"
I0530 14:56:49.609952       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0530 14:56:49.631296       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0530 14:56:49.656746       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0530 14:56:49.678918       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0530 14:56:49.703055       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0530 14:56:50.122161       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0530 14:56:50.152976       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0530 14:56:50.152993       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0530 14:56:50.152998       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"


==> kube-proxy [40f801b9e372] <==
I0530 14:56:51.353229       1 server_linux.go:63] "Using iptables proxy"
I0530 14:56:51.606323       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0530 14:56:51.606540       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0530 14:56:51.635402       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0530 14:56:51.635486       1 server_linux.go:145] "Using iptables Proxier"
I0530 14:56:51.645432       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0530 14:56:51.655706       1 proxier.go:271] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0530 14:56:51.664102       1 proxier.go:271] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0530 14:56:51.664240       1 server.go:516] "Version info" version="v1.33.1"
I0530 14:56:51.664267       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0530 14:56:51.672052       1 metrics.go:379] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0530 14:56:51.678634       1 metrics.go:379] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0530 14:56:51.679774       1 config.go:199] "Starting service config controller"
I0530 14:56:51.679827       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0530 14:56:51.679882       1 config.go:105] "Starting endpoint slice config controller"
I0530 14:56:51.679900       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0530 14:56:51.679884       1 config.go:329] "Starting node config controller"
I0530 14:56:51.679993       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0530 14:56:51.680086       1 config.go:440] "Starting serviceCIDR config controller"
I0530 14:56:51.680124       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0530 14:56:51.780602       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0530 14:56:51.780643       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"
I0530 14:56:51.780665       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0530 14:56:51.780629       1 shared_informer.go:357] "Caches are synced" controller="service config"


==> kube-scheduler [b2b7ffc75a1e] <==
I0530 14:56:41.845849       1 serving.go:386] Generated self-signed cert in-memory
W0530 14:56:43.422936       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0530 14:56:43.422972       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0530 14:56:43.422985       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0530 14:56:43.422994       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0530 14:56:43.436788       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0530 14:56:43.436842       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0530 14:56:43.438994       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0530 14:56:43.439094       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0530 14:56:43.439193       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0530 14:56:43.439483       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E0530 14:56:43.442033       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0530 14:56:43.442166       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0530 14:56:43.442295       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0530 14:56:43.442315       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0530 14:56:43.442433       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0530 14:56:43.442522       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0530 14:56:43.442556       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0530 14:56:43.442618       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0530 14:56:43.442670       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0530 14:56:43.442717       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0530 14:56:43.442757       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0530 14:56:43.442800       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0530 14:56:43.442884       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0530 14:56:43.442957       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0530 14:56:43.443062       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0530 14:56:43.443048       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0530 14:56:44.290930       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0530 14:56:44.306906       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0530 14:56:44.462733       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
I0530 14:56:47.239791       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
May 30 14:56:46 minikube kubelet[2698]: I0530 14:56:46.268472    2698 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
May 30 14:56:46 minikube kubelet[2698]: I0530 14:56:46.268494    2698 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
May 30 14:56:46 minikube kubelet[2698]: I0530 14:56:46.268525    2698 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
May 30 14:56:46 minikube kubelet[2698]: I0530 14:56:46.268581    2698 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
May 30 14:56:46 minikube kubelet[2698]: I0530 14:56:46.268611    2698 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
May 30 14:56:46 minikube kubelet[2698]: I0530 14:56:46.268626    2698 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
May 30 14:56:46 minikube kubelet[2698]: I0530 14:56:46.268639    2698 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
May 30 14:56:46 minikube kubelet[2698]: I0530 14:56:46.268660    2698 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
May 30 14:56:46 minikube kubelet[2698]: I0530 14:56:46.268683    2698 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
May 30 14:56:46 minikube kubelet[2698]: I0530 14:56:46.268720    2698 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
May 30 14:56:46 minikube kubelet[2698]: I0530 14:56:46.268746    2698 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/3924ef3609584191d8d09190210d2d78-etcd-certs\") pod \"etcd-minikube\" (UID: \"3924ef3609584191d8d09190210d2d78\") " pod="kube-system/etcd-minikube"
May 30 14:56:46 minikube kubelet[2698]: I0530 14:56:46.268778    2698 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
May 30 14:56:46 minikube kubelet[2698]: I0530 14:56:46.268799    2698 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/feee622ba49882ef945e2406d3ba86df-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"feee622ba49882ef945e2406d3ba86df\") " pod="kube-system/kube-scheduler-minikube"
May 30 14:56:46 minikube kubelet[2698]: I0530 14:56:46.859738    2698 apiserver.go:52] "Watching apiserver"
May 30 14:56:46 minikube kubelet[2698]: I0530 14:56:46.921950    2698 desired_state_of_world_populator.go:158] "Finished populating initial desired state of world"
May 30 14:56:47 minikube kubelet[2698]: I0530 14:56:47.034140    2698 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
May 30 14:56:47 minikube kubelet[2698]: I0530 14:56:47.034319    2698 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
May 30 14:56:47 minikube kubelet[2698]: I0530 14:56:47.034452    2698 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
May 30 14:56:47 minikube kubelet[2698]: I0530 14:56:47.034595    2698 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
May 30 14:56:47 minikube kubelet[2698]: E0530 14:56:47.053104    2698 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
May 30 14:56:47 minikube kubelet[2698]: E0530 14:56:47.054888    2698 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
May 30 14:56:47 minikube kubelet[2698]: E0530 14:56:47.055192    2698 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
May 30 14:56:47 minikube kubelet[2698]: E0530 14:56:47.055414    2698 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
May 30 14:56:47 minikube kubelet[2698]: I0530 14:56:47.147613    2698 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=2.147592008 podStartE2EDuration="2.147592008s" podCreationTimestamp="2025-05-30 14:56:45 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-05-30 14:56:47.132693328 +0000 UTC m=+1.352797365" watchObservedRunningTime="2025-05-30 14:56:47.147592008 +0000 UTC m=+1.367696045"
May 30 14:56:47 minikube kubelet[2698]: I0530 14:56:47.223815    2698 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=2.223781481 podStartE2EDuration="2.223781481s" podCreationTimestamp="2025-05-30 14:56:45 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-05-30 14:56:47.14785572 +0000 UTC m=+1.367959757" watchObservedRunningTime="2025-05-30 14:56:47.223781481 +0000 UTC m=+1.443885618"
May 30 14:56:47 minikube kubelet[2698]: I0530 14:56:47.323442    2698 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=2.323416522 podStartE2EDuration="2.323416522s" podCreationTimestamp="2025-05-30 14:56:45 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-05-30 14:56:47.22465412 +0000 UTC m=+1.444758257" watchObservedRunningTime="2025-05-30 14:56:47.323416522 +0000 UTC m=+1.543520659"
May 30 14:56:47 minikube kubelet[2698]: I0530 14:56:47.345402    2698 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=2.345382324 podStartE2EDuration="2.345382324s" podCreationTimestamp="2025-05-30 14:56:45 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-05-30 14:56:47.324545574 +0000 UTC m=+1.544649711" watchObservedRunningTime="2025-05-30 14:56:47.345382324 +0000 UTC m=+1.565486361"
May 30 14:56:49 minikube kubelet[2698]: I0530 14:56:49.642497    2698 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-h6d6w\" (UniqueName: \"kubernetes.io/projected/fe05da76-4266-42a2-b906-d2fffcc46040-kube-api-access-h6d6w\") pod \"storage-provisioner\" (UID: \"fe05da76-4266-42a2-b906-d2fffcc46040\") " pod="kube-system/storage-provisioner"
May 30 14:56:49 minikube kubelet[2698]: I0530 14:56:49.642591    2698 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/fe05da76-4266-42a2-b906-d2fffcc46040-tmp\") pod \"storage-provisioner\" (UID: \"fe05da76-4266-42a2-b906-d2fffcc46040\") " pod="kube-system/storage-provisioner"
May 30 14:56:49 minikube kubelet[2698]: E0530 14:56:49.747187    2698 projected.go:289] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
May 30 14:56:49 minikube kubelet[2698]: E0530 14:56:49.747228    2698 projected.go:194] Error preparing data for projected volume kube-api-access-h6d6w for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
May 30 14:56:49 minikube kubelet[2698]: E0530 14:56:49.747349    2698 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/fe05da76-4266-42a2-b906-d2fffcc46040-kube-api-access-h6d6w podName:fe05da76-4266-42a2-b906-d2fffcc46040 nodeName:}" failed. No retries permitted until 2025-05-30 14:56:50.247329512 +0000 UTC m=+4.467433549 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-h6d6w" (UniqueName: "kubernetes.io/projected/fe05da76-4266-42a2-b906-d2fffcc46040-kube-api-access-h6d6w") pod "storage-provisioner" (UID: "fe05da76-4266-42a2-b906-d2fffcc46040") : configmap "kube-root-ca.crt" not found
May 30 14:56:50 minikube kubelet[2698]: I0530 14:56:50.447238    2698 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/05c08aba-62cf-4c04-8789-c989b5e4081d-xtables-lock\") pod \"kube-proxy-nqbxq\" (UID: \"05c08aba-62cf-4c04-8789-c989b5e4081d\") " pod="kube-system/kube-proxy-nqbxq"
May 30 14:56:50 minikube kubelet[2698]: I0530 14:56:50.447283    2698 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/05c08aba-62cf-4c04-8789-c989b5e4081d-lib-modules\") pod \"kube-proxy-nqbxq\" (UID: \"05c08aba-62cf-4c04-8789-c989b5e4081d\") " pod="kube-system/kube-proxy-nqbxq"
May 30 14:56:50 minikube kubelet[2698]: I0530 14:56:50.447300    2698 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/05c08aba-62cf-4c04-8789-c989b5e4081d-kube-proxy\") pod \"kube-proxy-nqbxq\" (UID: \"05c08aba-62cf-4c04-8789-c989b5e4081d\") " pod="kube-system/kube-proxy-nqbxq"
May 30 14:56:50 minikube kubelet[2698]: I0530 14:56:50.447314    2698 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-ndnr7\" (UniqueName: \"kubernetes.io/projected/05c08aba-62cf-4c04-8789-c989b5e4081d-kube-api-access-ndnr7\") pod \"kube-proxy-nqbxq\" (UID: \"05c08aba-62cf-4c04-8789-c989b5e4081d\") " pod="kube-system/kube-proxy-nqbxq"
May 30 14:56:50 minikube kubelet[2698]: I0530 14:56:50.648465    2698 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/bd98a7db-2dc6-4ffb-b43f-488dd121fac1-config-volume\") pod \"coredns-674b8bbfcf-b92tm\" (UID: \"bd98a7db-2dc6-4ffb-b43f-488dd121fac1\") " pod="kube-system/coredns-674b8bbfcf-b92tm"
May 30 14:56:50 minikube kubelet[2698]: I0530 14:56:50.648555    2698 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-bhvz4\" (UniqueName: \"kubernetes.io/projected/bd98a7db-2dc6-4ffb-b43f-488dd121fac1-kube-api-access-bhvz4\") pod \"coredns-674b8bbfcf-b92tm\" (UID: \"bd98a7db-2dc6-4ffb-b43f-488dd121fac1\") " pod="kube-system/coredns-674b8bbfcf-b92tm"
May 30 14:56:52 minikube kubelet[2698]: I0530 14:56:52.175544    2698 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=5.175517496 podStartE2EDuration="5.175517496s" podCreationTimestamp="2025-05-30 14:56:47 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-05-30 14:56:52.175217183 +0000 UTC m=+6.395321220" watchObservedRunningTime="2025-05-30 14:56:52.175517496 +0000 UTC m=+6.395621533"
May 30 14:56:52 minikube kubelet[2698]: I0530 14:56:52.175697    2698 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-nqbxq" podStartSLOduration=2.175688604 podStartE2EDuration="2.175688604s" podCreationTimestamp="2025-05-30 14:56:50 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-05-30 14:56:52.147870536 +0000 UTC m=+6.367974673" watchObservedRunningTime="2025-05-30 14:56:52.175688604 +0000 UTC m=+6.395792741"
May 30 14:56:53 minikube kubelet[2698]: I0530 14:56:53.882979    2698 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-674b8bbfcf-b92tm" podStartSLOduration=3.88295661 podStartE2EDuration="3.88295661s" podCreationTimestamp="2025-05-30 14:56:50 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-05-30 14:56:52.222665945 +0000 UTC m=+6.442769982" watchObservedRunningTime="2025-05-30 14:56:53.88295661 +0000 UTC m=+8.103060647"
May 30 14:56:56 minikube kubelet[2698]: I0530 14:56:56.177937    2698 kuberuntime_manager.go:1746] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
May 30 14:56:56 minikube kubelet[2698]: I0530 14:56:56.179034    2698 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
May 30 14:56:56 minikube kubelet[2698]: I0530 14:56:56.545193    2698 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
May 30 14:57:14 minikube kubelet[2698]: I0530 14:57:14.296509    2698 scope.go:117] "RemoveContainer" containerID="3e6da2a0a970261f5f509e50ecf6d6f4dea2688bef17130b27314e739c2affab"
May 30 14:58:01 minikube kubelet[2698]: I0530 14:58:01.510866    2698 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-4rv6j\" (UniqueName: \"kubernetes.io/projected/926634e0-79c1-4588-a33b-518ffcfc17ff-kube-api-access-4rv6j\") pod \"flaskapp-deployment-55d46dc985-kv4ws\" (UID: \"926634e0-79c1-4588-a33b-518ffcfc17ff\") " pod="default/flaskapp-deployment-55d46dc985-kv4ws"
May 30 14:58:05 minikube kubelet[2698]: E0530 14:58:05.466003    2698 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for flaskapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="flaskapp:latest"
May 30 14:58:05 minikube kubelet[2698]: E0530 14:58:05.466071    2698 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for flaskapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="flaskapp:latest"
May 30 14:58:05 minikube kubelet[2698]: E0530 14:58:05.466343    2698 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:flaskapp,Image:flaskapp,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4rv6j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod flaskapp-deployment-55d46dc985-kv4ws_default(926634e0-79c1-4588-a33b-518ffcfc17ff): ErrImagePull: Error response from daemon: pull access denied for flaskapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 30 14:58:05 minikube kubelet[2698]: E0530 14:58:05.467600    2698 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flaskapp\" with ErrImagePull: \"Error response from daemon: pull access denied for flaskapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flaskapp-deployment-55d46dc985-kv4ws" podUID="926634e0-79c1-4588-a33b-518ffcfc17ff"
May 30 14:58:05 minikube kubelet[2698]: E0530 14:58:05.622597    2698 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flaskapp\" with ImagePullBackOff: \"Back-off pulling image \\\"flaskapp\\\": ErrImagePull: Error response from daemon: pull access denied for flaskapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flaskapp-deployment-55d46dc985-kv4ws" podUID="926634e0-79c1-4588-a33b-518ffcfc17ff"
May 30 14:58:22 minikube kubelet[2698]: E0530 14:58:22.056798    2698 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for flaskapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="flaskapp:latest"
May 30 14:58:22 minikube kubelet[2698]: E0530 14:58:22.056915    2698 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for flaskapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="flaskapp:latest"
May 30 14:58:22 minikube kubelet[2698]: E0530 14:58:22.057202    2698 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:flaskapp,Image:flaskapp,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4rv6j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod flaskapp-deployment-55d46dc985-kv4ws_default(926634e0-79c1-4588-a33b-518ffcfc17ff): ErrImagePull: Error response from daemon: pull access denied for flaskapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 30 14:58:22 minikube kubelet[2698]: E0530 14:58:22.058504    2698 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flaskapp\" with ErrImagePull: \"Error response from daemon: pull access denied for flaskapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flaskapp-deployment-55d46dc985-kv4ws" podUID="926634e0-79c1-4588-a33b-518ffcfc17ff"
May 30 14:58:34 minikube kubelet[2698]: E0530 14:58:34.931758    2698 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flaskapp\" with ImagePullBackOff: \"Back-off pulling image \\\"flaskapp\\\": ErrImagePull: Error response from daemon: pull access denied for flaskapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flaskapp-deployment-55d46dc985-kv4ws" podUID="926634e0-79c1-4588-a33b-518ffcfc17ff"
May 30 14:58:52 minikube kubelet[2698]: E0530 14:58:52.788678    2698 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for flaskapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="flaskapp:latest"
May 30 14:58:52 minikube kubelet[2698]: E0530 14:58:52.788741    2698 kuberuntime_image.go:42] "Failed to pull image" err="Error response from daemon: pull access denied for flaskapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="flaskapp:latest"
May 30 14:58:52 minikube kubelet[2698]: E0530 14:58:52.788858    2698 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:flaskapp,Image:flaskapp,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4rv6j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod flaskapp-deployment-55d46dc985-kv4ws_default(926634e0-79c1-4588-a33b-518ffcfc17ff): ErrImagePull: Error response from daemon: pull access denied for flaskapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 30 14:58:52 minikube kubelet[2698]: E0530 14:58:52.790084    2698 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"flaskapp\" with ErrImagePull: \"Error response from daemon: pull access denied for flaskapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/flaskapp-deployment-55d46dc985-kv4ws" podUID="926634e0-79c1-4588-a33b-518ffcfc17ff"


==> storage-provisioner [3e6da2a0a970] <==
I0530 14:56:51.080580       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0530 14:57:12.308060       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused


==> storage-provisioner [47cce1663965] <==
W0530 14:57:56.883532       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:57:56.888863       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:57:58.891695       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:57:58.895921       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:00.899871       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:00.915478       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:02.919316       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:02.924094       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:04.927383       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:04.935339       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:06.938356       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:06.942366       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:08.945958       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:08.964101       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:10.967532       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:10.973783       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:12.976671       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:12.981343       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:14.985261       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:14.991267       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:16.994051       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:16.998787       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:19.001856       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:19.006173       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:21.011413       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:21.017844       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:23.020733       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:23.024935       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:25.028035       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:25.034982       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:27.040010       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:27.045012       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:29.056744       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:29.061953       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:31.064907       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:31.071390       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:33.074379       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:33.080175       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:35.083546       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:35.089522       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:37.092349       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:37.099103       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:39.102182       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:39.106610       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:41.109418       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:41.147269       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:43.153088       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:43.176912       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:45.180029       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:45.185758       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:47.200895       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:47.270932       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:50.796108       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:50.902332       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:52.906839       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:52.913789       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:54.917474       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:54.923554       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:56.927037       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0530 14:58:56.933568       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

